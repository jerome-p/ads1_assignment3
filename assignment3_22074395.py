import cluster_tools as clst
import pandas as pd
import sklearn.cluster as cluster
import matplotlib.pyplot as plt
import sklearn.metrics as skmet
import numpy as np
import scipy.optimize as opt
import errors as err

print(help(err))


def load_data(dataset, country_list):
    """
    This function takes in a csv file and a list of countries that are of
    interest. Returns two dataframes one with the years as columns and the
    other country names as columns. (Reused assignment_2's code to
                                     load dataset')

    Parameters
    ----------
    dataset : .csv file
    country_list : List
    
    Returns DF and transposed DF
    """
    # skipping first 4 rows, as they contain non essential data.
    world_bank_df = pd.read_csv(dataset, skiprows=4)

    # Removing non essential data.
    world_bank_df.drop(['Country Code', 'Indicator Code', 'Unnamed: 67'],
                       axis=1, inplace=True)

    # subsetting the dataframe to get data for countries we are interested in.
    world_bank_df = world_bank_df[
        world_bank_df['Country Name'].isin(country_list)]

    # Setting index before transposing the dataframe
    temp_df = world_bank_df.set_index('Country Name')

    return world_bank_df, temp_df.T


def subset_data(data,
                indicator1,indicator2,indicator3,indicator4,
                countries):
    """
    This function returns a susbset of the given dataframe. It returns a 
    dataframe with indicator values for each country appended column wise
    along with the country's name for each value.
    
    Returns DF

    """    
    # Selecting only the indicators needed from the dataset.
    data = data[
        (data['Indicator Name'] == indicator1) |
        (data['Indicator Name'] == indicator2) |
        (data['Indicator Name'] == indicator3) |
        (data['Indicator Name'] == indicator4)
    ]      
    
    result_df = pd.DataFrame()
    for country in countries:
        temp = data[data['Country Name'] == country]
        
        # Dropping Country Name column to make it easier to label
        # and the columns 1960 and 2022 have Na values
        temp = temp.drop(['Country Name', '1960', '2022'], axis=1)
        temp = temp.set_index('Indicator Name')
        temp = temp.T
        temp['Country'] = country
        result_df = pd.concat([result_df, temp], axis=0, ignore_index=True)
        
    return result_df


def sil_score(normalised_df, indicator):
    """
    This function generates the silhouette score for the given dataframe. It 
    uses the sklearn metrics module to generate the score. Using which we 
    the number of clusters can be decided.
    
    Parameters: DataFrame
    
    Returns:
        int, with the optimal number of clusters       
    
    """
    #Empty list to store obtained scores
    score =[]
    for n in range(2,8):
        #passing multiple values of n using for loop to kmeans
        kmeans = cluster.KMeans(n_clusters=n, n_init=20)
        #Fitting the normalised_df to the kmeans model with multiple 
        #values of n to find the optimal number of clusters.
        kmeans.fit(normalised_df)
        labels = kmeans.labels_
        #Storing the obtained silhouette scores into a list
        score.append(skmet.silhouette_score(normalised_df, labels))
        
    # Converting list to numpy array to use argmax()
    # Using which, the returned index will be used to get the cluster number 
    # from variable x. Since both score and x have same length.
    s = np.array(score)
    
    #Creating variable for x-axis of the plot
    x = range(2,8)
    
    #Plotting the different scores
    plt.figure()
    plt.plot(x, score, label="Optimal no.of clusters: "+str(x[s.argmax()]))
    
    #Labelling
    plt.xlabel('no.of clusters')
    plt.ylabel('score')
    plt.title("Silhouette Score")
    plt.legend()
    plt.savefig("figures/Silhouette_score_"+indicator+".png", dpi=300)
    plt.show()
    
    #Returning the best number of clusters
    return x[s.argmax()]
        

def generate_kmeans_cluster_plot(result,normalised_df, indicator_1, 
                                 indicator_2, ncluster,
                                 min_values, max_values,
                                 ylabel):
    """
    This function generates a scatter plot using the kmeans algortihm. It 
    It helps visualise clusters of data. 
    
    Parameters: 
        result: DF
        normalised_df: DF
        indicator_1: String
        indicator_2: String
        ncluster: int
        max_values,min_values: Outputs from cluster tools scaler function.
        
    Returns: None
    
    """
    cluster.KMeans.OMP_NUM_THREADS=1
    #performing kmeans clustering with the number of clusters provided.
    kmeans = cluster.KMeans(n_clusters=ncluster, n_init=20)
    
    #Fitting using normalised values
    kmeans.fit(normalised_df)
    #storing the labels generated by the algorithm
    labels = kmeans.labels_
    #Getting the centers of the clusters
    cen = kmeans.cluster_centers_
    #x and y values for the cluster centers. This is used to plot the centers
    xkmeans = cen[:,0]
    ykmeans = cen[:,1]
    #storing the labels into the result DF. The labels will be used to extract
    #country names using logical slicing.
    result['label'] = labels
    #Colormap
    cm = plt.colormaps["Paired"]
    
    plt.figure()
    #Plotting the clusters using normalised values
    plt.scatter(normalised_df[indicator_1], 
                normalised_df[indicator_2], marker="o", c=labels,
                cmap=cm)
    #plotting the kmeans centers
    plt.scatter(xkmeans, ykmeans, marker="d", label="kmeans centres", c='red')
    
    #Labeling
    plt.xlabel(indicator_1)
    plt.ylabel(ylabel)
    plt.legend()
    plt.savefig("figures/kmeans_cluster_"+ylabel+".png",dpi=300)
    plt.show()

    #New figure, to generate clusters using the real values
    plt.figure()
    #Backscaling clusters to fit the real values
    backscaled_centers = clst.backscale(cen, min_values, max_values)
    x = backscaled_centers[:,0]
    y = backscaled_centers[:,1]
    
    #Getting the list of clusters generated by Kmeans algorithm.
    #Using logical slicing to extract the desired labels and the country 
    #name associated to that row.
    cluster_name = []
    for i in range(ncluster):
        cluster_name.append(result[result['label'] == i]['Country'].unique())
      
    #Creating an empty list using which we can store country names 
    #This list will be used to generate cluster label
    country_list = []    
    #Empty string, in which the country names will be appended.
    country_name = ""
    
    #Looping throught the
    for i in range(len(cluster_name)):
        for j in range(len(cluster_name[i])):
            country_name += cluster_name[i][j] + ", "
        country_list.append(country_name)
        country_name = ""
        
    #Printing what countries Kmeans has clustered together.   
    for i in range(len(country_list)):
        print("Kmeans Label "+str(i)+": "+ country_list[i])
            
    #Plotting each cluster for loop, this is being done so that the label 
    #can give us information for each cluster.    
    for i in result['label'].unique():
        plt.scatter(result[result['label'] == i][indicator_1],
                    result[result['label'] == i][indicator_2],
                    label=country_list[i])
   
    #Plotting original centers     
    plt.scatter(x, y, marker="d", label="original centres", c='black' )
    
    #Labelling
    plt.title('Clusters for '+ylabel)
    plt.xlabel(indicator_1)
    plt.ylabel(ylabel)    
    plt.legend()
    plt.savefig("figures/cluster_"+ylabel+".png", dpi=300)
    plt.show()    
    
    return 
    

def exp_growth(t, scale, growth):
    """
    Computes exponential function with scale and growth as free parameters
    """
    
    f = scale * np.exp(growth * (t-1960)) 
    
    return f


def logistics(t, a, k, t0):
    """ Computes logistics function with scale and incr as free parameters
    """
    
    f = a / (1.0 + np.exp(-k * (t - t0)))
    
    return f


def curve_fit_plot(data, country, indicator, xlabel, ylabel, title, funct,p=None):
    """
    This function fits a exponential function to a given data. In this case,
    the values of an indicator for a selected country. It can perform 
    curve fitting with or without inital parameters. Default is to perform
    curve fitting wihtout initlal parameters
    
    Returns:
    DataFrame with original values, fitted/predicted values and error range
    DataFrame with predictions for years 2023 to 2026

    """
    temp_df = data[country].T
    # Subsetting the transposed df. Which now has years as columns
    subset_df = temp_df[temp_df['Indicator Name'] == indicator]
    # Transposing the df again to makes years the index.
    subset_df = subset_df.T
    
    #Manipulating the dataframe to get it into the required format
    #Resetting the index
    subset_df = subset_df.reset_index()
    #Dropping the first row
    subset_df = subset_df.drop(index=0)
    
    #Renaming the index to make it convinient to select data.
    subset_df = subset_df.rename(
        columns={'index': 'Year', 
                 country: indicator})
    
    #Converting the year data from string to int
    xdata = subset_df['Year'].astype(int)
    

    #Converting the selected indicator data to float type
    ydata = subset_df[indicator].astype(float)
    #Fitting the funct function
    if p:
        param, pcovar = opt.curve_fit(funct, xdata, ydata, p0=p)  
    else:
        param, pcovar = opt.curve_fit(funct, xdata, ydata)  

    #Storing the predicted values from the fitted model
    subset_df['prediction'] = funct(xdata, *param)
    
    fit_data = subset_df['prediction'].astype(float)
        
    #Plotting the orginal data
    plt.figure()
    plt.plot(xdata,
             ydata, 
             label='Data')
    #Plotting the fitted data
    plt.plot(xdata, 
             fit_data, 
             label='fit')
    
    #Labelling
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend()
    plt.title(title)
    plt.savefig("figures/curve_fit_"+title+".png", dpi=300)
    plt.show()
    
    # Visualising Error for the fitted data, using the errors py file
    sigma = err.error_prop(xdata, funct, param, pcovar)
    
    errors = sigma
    #Calculating upper and lower ranges / confidence interval
    low = fit_data - sigma
    up = fit_data + sigma
    
    #Plotting data with errors ranges
    plt.figure()
    plt.title(title+" Errors Visualised")
    plt.plot(xdata, ydata, label="Data")
    plt.plot(xdata, fit_data, label="fit")
    # plot error ranges with transparency
    plt.fill_between(xdata, low, up, alpha=0.5, color="yellow")
    plt.errorbar(xdata, fit_data, errors,
                 color="black", linestyle="none")
    #Labelling
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend()
    plt.savefig("figures/curve_fit_error_"+title+".png", dpi=300)
    plt.show()
        

    
    #Visualising error ranges for the last 10 rows
    plt.figure()
    plt.title(title+" Errors Visualised last 10 rows")
    plt.plot(xdata[-10:-1], ydata[-10:-1], label="Data")
    plt.plot(xdata[-10:-1], fit_data[-10:-1], label="fit")
    # plot error ranges with transparency
    plt.fill_between(xdata[-10:-1], low[-10:-1], up[-10:-1], alpha=0.5,
                     color="yellow")
    #Plotting errorbars
    plt.errorbar(xdata[-10:-1], fit_data[-10:-1], errors[-10:-1],
                 color="black", linestyle="none")

    #Labelling
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend()
    plt.savefig("figures/curve_fit_error_10_rows"+title+".png", dpi=300)
    plt.show()
    
    #Storing the error range 
    subset_df['Error Range +/-'] = errors
    
    #Creating a Dataframe with years to predict the selected indicator
    predictions = pd.DataFrame([2022,2023,2024,2025,2026], columns=['Year'])
    pred = funct(predictions['Year'], *param)
    
    #Storing the prediction
    predictions['Prediction'] = pred.astype(float)
    #Storing the error range / confidence interval
    pred_errors = err.error_prop(predictions['Year'], funct, param, pcovar)
    predictions['Error Range'] = pred_errors
    
    #Calculating upper and lower error range/ confidence interval
    pred_low = predictions['Prediction'] - pred_errors
    pred_up = predictions['Prediction'] + pred_errors

    # Plotting the predictions
    plt.figure()
    
    plt.plot(predictions['Year'], predictions['Prediction'])
    plt.fill_between(predictions['Year'], pred_low, pred_up, 
                     alpha=0.5, color="green")
    
    # Fixing the xlabel. https://stackoverflow.com/a/12608937
    plt.xticks(np.arange(min(predictions['Year']), 
                         max(predictions['Year'])+1, 1.0))
    #labelling
    plt.title("Prediction with highlighted confidence interval")
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)   
    plt.savefig("figures/curve_fit_predcition"+title+".png", dpi=300)
    plt.show()
    
    return subset_df, predictions


def polynomial_func(x,a,b,c):
    return a*x**2 + b*x + c


def main():
    
    # Create a list of Countries interested in evaluating.
    countries = ['China', 'India', 'Japan',
                 'United Kingdom', 'United States', 'Germany']
    #loading dataset
    wb_data_years, wb_data_country = load_data(
        'climate_change.csv',
        countries)

    #Subsetting the dataset and getting it into desired format
    result = subset_data(wb_data_years,
                'CO2 emissions from solid fuel consumption (% of total)',
                'CO2 emissions (kt)',
                'Urban population (% of total population)',
                'Electric power consumption (kWh per capita)',
                countries) 
    #dropping Na values along row, as kmeans does not work with na values
    result = result.dropna(axis=0)
    
    
    # This produces a heatmap using cluster_tools py file.
    # The generated plot had to be saved maunally from spyder IDE.
    clst.map_corr(result)
    
    
    #-----------------------------Cluster 1----------------------------------
    # Choosing indicators based on the heatmap
    indicator_1 = 'Urban population (% of total population)'
    indicator_2 = 'CO2 emissions from solid fuel consumption (% of total)'
    
    cluster_df = result[[indicator_1, indicator_2]]    
    scaler_output = clst.scaler(cluster_df)
    
    normalised_df = scaler_output[0]
    #Extracting min and max values from scaler ouput. 
    min_values = scaler_output[1]
    max_values = scaler_output[2]
    #Calculating siloheute score and choosing no.of clusters
    n_cluster = sil_score(normalised_df, 
                          indicator_2)
    #Generating kmeans clusters
    generate_kmeans_cluster_plot(result,normalised_df,
                                indicator_1, indicator_2, 
                                n_cluster, min_values, max_values,
                                "CO2 % of total, from solid fuel consumption")
    
    
    #-----------------------------Cluster 2----------------------------------
    # Choosing indicators based on the heatmap
    indicator_1 = 'Urban population (% of total population)'
    indicator_2 = 'Electric power consumption (kWh per capita)'
    
    cluster_df = result[[indicator_1, indicator_2]]
    #Using the cluster tool py file to normalise and get min and max values
    scaler_output = clst.scaler(cluster_df)
    normalised_df = scaler_output[0]
    min_values = scaler_output[1]
    max_values = scaler_output[2]
    
    #Calculating siloheute score and choosing no.of clusters
    n_cluster = sil_score(normalised_df, indicator_2)
    
    #generating kmeans cluster plot
    generate_kmeans_cluster_plot(result,normalised_df,
                                 indicator_1, indicator_2, 
                                 n_cluster, min_values, 
                                 max_values, indicator_2)

    
    #---------------------------Curve Fitting--------------------------------

    #Fitting a exponential function for India's Urban population %
    DF1, predictions1 = curve_fit_plot(wb_data_country,
                       'India',
                       'Urban population (% of total population)',
                       'Years',
                       'Urban population (%)', 
                       'India - exponential function (0,0) ',
                       exp_growth,
                       (0,0)
                       )
    
    
    DF2, predictions2 = curve_fit_plot(wb_data_country,
                       'India',
                       'Urban population (% of total population)',
                       'Years',
                       'Urban population (%)',
                       'Polynomial Function ',
                       polynomial_func,
                       )
        
    DF3, predictions3  = curve_fit_plot(wb_data_country,
                       'India',
                       'Urban population (% of total population)',
                       'Years','Urban population (%)',
                       'Logistics function (17,0,0)',
                       logistics,
                       (17,0,0)
                       )
    
    return DF1, DF2, DF3, predictions1, predictions2, predictions3


if __name__ == '__main__':
    # variables to access predictions and error ranges
    df1, df2, df3, prediction1, prediction2, prediction3 = main()